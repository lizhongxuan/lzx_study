# 性能优化案例 - 架构设计

> **导航**：[🔙 返回目录页](./README.md)
---
### 案例一：基于多级缓存架构消除热点 Key 瓶颈

####   科普的简介

> 我对**首页高频配置读取**场景下，做了**多级缓存架构**的优化。主要是针对**单一热点 Key 把 Redis 网卡打满**的问题，借鉴 CPU L1/L2 缓存思想，引入了**进程内缓存 (Local Cache)**。最终达到的效果是**将 90% 的流量拦截在应用内部，将热点读取延迟从 50ms 降低到了 0.1ms**。

#### 背景

在大促期间，所有用户请求都会读取同一个“活动配置” Key。监控显示，Redis 单个分片的网卡带宽被打满，序列化开销巨大，导致 P99 延迟飙升到 50ms，甚至拖累了同一分片下的其他核心业务。

#### 实现逻辑

1. **多级架构：** 构建 App 内部 (L1) -> Redis (L2) -> DB (L3) 的读取链路。
2. **读写策略：** 请求先查本地 Map，有则直接返回；无则查 Redis 并回填本地 Map。
3. **内存优化：** 针对大量小对象场景，使用 Go 的 **`BigCache`**（基于堆外内存/字节数组），避免原生 Map 导致的 GC 扫描停顿（STW）。
4. **短 TTL：** 本地缓存设置极短的过期时间（如 2 秒），以保证数据最终一致性。

#### 难点

1. **GC 压力：** 如果简单地用原生 `map[string]interface{}` 存几百万个对象，Go 的 GC 标记阶段会非常慢，导致服务抖动。通过选用对 GC 友好的缓存库解决。
2. **数据一致性：** 本地缓存会导致各节点数据短暂不一致。我们经过业务评估，配置类数据允许 2-3 秒的延迟，因此采用了短 TTL 策略，而非复杂的广播失效策略。

#### 结果

1. **流量削峰：** Redis 的 QPS 下降 **90%**，网卡流量恢复正常。
2. **性能飞跃：** 热点数据读取变为纯内存拷贝，P99 延迟降至 **0.1ms**。

#### 思考

1. **底层原理：** 网络 IO 永远比内存操作慢几个数量级。性能优化的终极手段往往是**离数据更近一步**（空间局部性原理）。
2. **序列化成本：** 从 Redis 读数据需要 `JSON Unmarshal`，这其实非常消耗 CPU。本地缓存直接存结构体，省去了反复解析的成本，这是被很多人忽略的优化点。
