# 性能优化案例 - 数据库

> **导航**：[🔙 返回目录页](./README.md)
---
### 案例一：计算下推优化 (Computation Pushdown)

#### 跟面试官科普的简介

> 面试官，我在处理大规模监控数据查询的场景下，做了**计算下推**的优化。主要是针对**网络传输带宽瓶颈**（优化原理），核心是将聚合计算从查询节点下沉到存储节点，贯彻了**“移动计算比移动数据更便宜”**的分布式思想。最终达到的效果是查询速度提升了 10 倍，且支持的查询周期从 1 天扩展到了 7 天。

#### 背景

在推荐系统等业务中，指标维度极高（达到千万级 Series），业务方往往需要查看 7 天的趋势数据 。旧架构中，查询节点（Select Node）需要从存储节点拉取全量的原始数据（Raw Data）到本地内存进行聚合 。单次查询涉及数亿数据点，导致查询节点网卡打满、内存溢出，大跨度查询经常超时甚至失败 。

#### 实现逻辑

1.**算子下发**：改变数据流向，不再拉取原始数据。将 `sum`, `min`, `max`, `rate` 等聚合算子“下推”到存储节点（Storage Node）本地执行 。

2.**中间结果计算**：存储节点利用本地 CPU 并行计算出“中间结果”（Partial Result） 。


3.**最终合并**：查询节点仅接收极小的数据量（中间结果），并在本地做最终的合并计算 。


4.**特殊函数处理**：针对 `avg` 这种不可直接拆分的非加性函数，将其拆解为 `sum` 和 `count` 两个算子分别下推，最后在查询节点计算除法 。



#### 难点

1.**非加性函数处理**：如上所述，`avg` 等函数不能直接下推，需要设计拆解和重组逻辑 。


2.**数据分布一致性**：下推 `rate` 等窗口函数的前提是分片和序列分布稳定，需要确保高可用改造后的数据分布满足计算要求 。



#### 结果

1.**速度提升**：1 天内的数据查询耗时从超时/慢查询降低到 **3秒以内** 。


2.**范围扩大**：可查询时间范围从 1 天提升到了 **7 天**，满足了日常巡检需求 。



#### 思考

1.**资源权衡 (Trade-off)**：这个方案增加了存储节点的 CPU 负载。但因为监控存储通常是 I/O 密集型，CPU 利用率较低且有 Buffer，利用闲置 CPU 换取巨大的网络带宽节省是非常划算的 。



---
### 案例二：采集端预聚合优化 (Pre-aggregation)

#### 跟面试官科普的简介

> 面试官，针对网关日志这类**高基数（High-Cardinality）**场景，单纯的查询优化已经到了物理极限。我做了**采集端预聚合**的优化，原理是**“以空间换时间”**，在数据写入前就进行降维处理。最终效果是实现了秒级查询一个月的数据，解决了高维指标无法长期存储和查询的问题。

#### 背景

网关（Gateway）等核心组件产生大量高基数指标，维度组合达千万级 。即便开启了计算下推，扫描的数据量依然太大，查询 1 天以上数据经常超时或 OOM，无法支持月度报表 。

#### 实现逻辑

1.
**提前聚合**：在数据进入存储前的采集阶段（vmagent），根据配置规则将高基数指标（Raw Metrics）提前聚合成低基数指标（Pre-aggregated Metrics） 。


2.
**查询重写 (Query Rewrite)**：在查询层（Query Node）做自动拦截，当用户查询原始指标时，系统判断是否匹配预聚合规则 。


3.
**透明路由**：如果匹配规则，则**自动改写 SQL** 去查询预聚合后的低维度指标，用户无感知 。



#### 难点

1.
**对用户透明**：最难的不是聚合，而是让用户不需要修改 Grafana 面板或 SQL 语句，系统自动完成路由，这是提升体验的关键 。



#### 结果

1.
**极速响应**：所有复杂查询在 **1秒内** 完成 。


2.
**周期突破**：轻松支持 **30天** 以上的数据查询 。



#### 思考

1.
**成本控制**：预聚合会产生额外的序列数据，需要权衡存储成本。通常我们只针对高价值、高查询频次的核心指标开启此功能，而非全量开启 。
